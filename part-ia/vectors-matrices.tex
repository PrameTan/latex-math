\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Vectors and Matrices}
	\section{Complex Numbers}
		\subsection{Complex logarithm}
		
	\section{Vectors}
		\subsection{Vector Algebra in $\mathbb{R}^3$}
			This section will review algebra of vectors in $\RR^3$. They are usually regarded as an arrow, one with \textit{dimension} and \textit{length}. Starting from two points inside the space $\RR^3$, namely $P(p_1, p_2, p_3)$ and $Q(q_1, q_2, q_3)$ we may draw a vector from $P$ to $Q$, and is expressed by
			\begin{equation*}
				\overrightarrow{PQ} = (q_1 - p_1, q_2 - p_2, q_3 - p_3).
			\end{equation*}
			Generally let $\mathbf{u} = (u_1, u_2, u_3)$. This vector can also be written as a sum of unit vectors laying on the axis. Those unit vectors are
			\begin{equation*}
				\mathbf{i} = (1, 0, 0),\quad \mathbf{j} = (0, 1, 0),\text{ and } \mathbf{k} = (0, 0, 1).
			\end{equation*}
			And $\mathbf{u} = u_1\mathbf{i} + u_2\mathbf{j} +  u_3\mathbf{k}$. We can multiply vectors by a scalar, which is a real number, by
			\begin{equation*}
				\mu\mathbf{u} = (\mu u_1, \mu u_2, \mu u_3).
			\end{equation*}
			The usual properties of vectors should be familiar, that is $\mu(\mathbf{u} + \mathbf{v}) =\mu\mathbf{u} + \mu\mathbf{v}$, $(\mu + \lambda)\mathbf{u} = \mu\mathbf{u} + \lambda\mathbf{u}$, and $(\mu\lambda)\mathbf{u} = \mu(\lambda\mathbf{u})$. 
			
			
		\subsection{Vectors in $\RR^n$ and $\CC^n$}
			Let us consider vectors in $\RR^n$, the natural generalisation of $\RR^3$.
			\begin{definition}
				Using the standard basis $e_1, \ldots, e_n$ of $\RR^n$, if $x = \sum_j x_j e_j$ and $y = \sum_j y_j e_j$, we write
				\begin{equation*}
					x \cdot y = \sum_{j = 1}^{n} x_j y_j, \norm{x}^2 = x \cdot x = \sum_{j = 1}^{n} x^2_j,
				\end{equation*}
				and $x \perp y$ when $x\cdot y = 0$.
			\end{definition}
		Note that $\norm x = \norm {-x}$. The distance $\norm{x - y}$ between the points $x$ and $y$ is given by the natural extension of Pythagoras' theorem, and importantly, satisfies the \textit{triangle inequality}.

			\begin{equation}
			\norm{x - z} \leq \norm{x - y} + \norm{y - z}.
			\end{equation}

		To prove this assertion, it is sufficient to show that $\abs{x \cdot y} \leq \norm x \norm y$, so that we have $\norm{x + y} \leq \norm x + \norm y$, which readily implies the triangle inequality. Thus we seek to prove
		\begin{theorem}[the Cauchy-Schwarz inequality]
			For all $x, y \in \RR^n$,
			\begin{equation}
				\abs{x \cdot y} \leq \norm x \norm y.
			\end{equation}
			The equality holds if and only if $\norm x y = \pm \norm y x$, i.e. one vector is a multiple of one another. 
		\end{theorem}
		\begin{proof}
			Let $x = (x_1, \ldots, x_n)$ and $y = (y_1, \ldots, y_n)$. The equation holds true when $x = 0$ and when $y = 0$. So we assume that $\norm x \norm y > 0$.
			
			Consider the equation
			\begin{equation*}
				0 \leq \sum_{j = 1}^n \left(\norm x y_j - \norm y x_j\right)^2 = 2\norm x \norm y \left(\norm x \norm y - xy\right),
			\end{equation*}
			so $x \cdot y \leq \norm x \norm y$; similarly, put $ - x $ as $x$ and we have $- x \cdot y \leq \norm x\norm y$. Therefore $\abs{x \cdot y} \leq \norm x \norm y$. Equality holds if $\sum_{j = 1}^n \left(\norm x y_j - \norm y x_j\right)^2$ or $\sum_{j = 1}^n \left(\norm x y_j + \norm y x_j\right)^2$ is equal to zero, which implies $ \norm x y = \pm \norm y x$.
		\end{proof}
		Now we are sufficiently equipped with the tool to prove the triangle inequality for general $\RR^n$ space.
		\begin{theorem}[The triangle inequality for $\RR^n$]
			For all $x, y, z$ in $\RR^n$,
			\begin{equation}
			\norm{x - z} \leq \norm{x - y} + \norm{y - z}.
			\end{equation}
		\end{theorem}
		\begin{proof}
			Set $a = x - y$ and $b = y - z$. The inequality is equivalent to $\norm{a + b} \leq \norm{a} + \norm{b}$, which we seek to prove. Note that
			\begin{align*}
				\norm{a + b}^2 = \left(a + b\right)\cdot\left(a + b\right) & = {\norm a}^2 + {\norm b}^2 + 2 a \cdot b \\
				& \leq {\norm a}^2 + {\norm b}^2 + 2 \norm a \norm b = \left(\norm a + \norm b\right)^2.
			\end{align*}
			Taking square root on both sides we arrive at $\norm{a + b} \leq \norm{a} + \norm{b}$.
		\end{proof}
	
		\subsection{Concepts in linear algebra}
			
		\subsection{Suffix notation}
		
		\subsection{Vector product and triple product}
		
		\subsection{Solution of linear vector equations}
		
		\subsection{Applications}
		
	\section{Matrices}
		\begin{definition}
			An $n \times m$ matrix is an array of numbers of the form
			\begin{equation*}
				\begin{pmatrix}
				a_{11} & a_{12} & \ldots & a_{1m} \\
				a_{21} & a_{22} & \ldots & a_{2m} \\
				\vdots & \vdots & \ddots & \vdots \\
				a_{n1} & a_{n2} & \ldots & a_{nm}
				\end{pmatrix}.
			\end{equation*}
			Sometimes it will be denoted by $(a_{ij})$, where $a_{ij}$ is the general element of the matrix, the index $i$ stands for the \textit{row} and $j$ for the \textit{column} of the element.
		\end{definition}
			\begin{definition}
				An $n \times n$ matrix is called a \textit{square} matrix.
			\end{definition}
		\subsection{Algebra of matrices}
		
		\subsection{Determinant and trace}
			\begin{theorem}
				The \textit{trace} \index{Trace} of an $n \times n$ matrix $\mathbf{A}$, denoted $\tr \mathbf{A}$ is the sum of its diagonal entries, that is
				\begin{equation*}
					\tr \mathbf{A} = \sum_{i = 1}^{n} a_{ii.} = a_{11} + a_{22} + \cdots + a_{nn}.
				\end{equation*}
			\end{theorem}
			\begin{theorem}
				It is evident that, for two square matrices $\mbf{A}$ and $\mbf{B}$ with same dimension,
				\begin{equation*}
				\tr(\mbf{A + B}) = \tr\mbf A + \tr\mbf B.
				\end{equation*}
			\end{theorem}
		
%			But it is not so obvious that the following holds.
%			\begin{equation*}
%				\tr(\mbf{P}^{-1}\mbf{AP}) = \tr\mbf A
%			\end{equation*}
%			for any invertible $n\times n$ matrix $\mbf P$ and any matrix $\mbf{A}$. Less so of its importance which shall be used later.
			
			
		\subsection{Matrix as linear transformation}
			We start with the definition of linear transformations.
			\begin{definition}
				A map $\alpha \colon V \ra W$ between vector spaces $V$ and $W$ is \textit{linear} if, for all scalars $\lambda_1, \ldots, \lambda_n,$ and all vectors $v_1, \ldots, v_n$,
				\begin{equation*}
					\alpha\left(\lambda_1v_1 + \cdots + \lambda_n v_n\right) = \lambda_1\alpha(v_1) + \cdots + \lambda_n\alpha(v_n).
				\end{equation*}
			If $\alpha$ is linear we say that it is a \textit{linear transformation}, or a \textit{linear map}, if for all scalars $\lambda$ and all vectors $u$ and $v$, $\alpha(\lambda x) = \lambda\alpha(x)$ and $\alpha( x + y) = \alpha (x) + \alpha(y)$.
			\end{definition}
		The two definitions are equivalent.
			\begin{theorem}[Rank-nullity theorem]
				content...
			\end{theorem}
		
		\subsection{Simultaneous linear equations}
		
		\section{Eigenvalues and Eigenvectors}
\end{document}