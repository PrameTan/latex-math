\documentclass[12pt]{book}

\usepackage{xltxtra, amsmath, xfrac,fontspec, amsfonts, mathtools, enumitem, indentfirst} 
\usepackage{amsthm}
\usepackage{tabu}

\setlength{\parindent}{0.5in}
\usepackage{multicol}
\setlength{\columnsep}{1cm}

\usepackage{color}   %May be necessary if you want to color links
\definecolor{maroon}{rgb}{0.5,0,0}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true, %set true if you want colored links
	linktoc=all,     %set to all if you want both sections and subsections linked
	linkcolor=maroon,  %choose some color if you want links to stand out
}

\usepackage{fancyhdr}

\usepackage{imakeidx}
\makeindex[title=Index]
%\usepackage[T1]{fontenc}
%\XeTeXlinebreaklocale "th"
%\XeTeXlinebreakskip = 0pt plus 0pt   %
%\sloppy

%\defaultfontfeatures{Mapping=tex-text} 
%\setmainfont{TeX Gyre Termes}	
%\setsansfont{TeX Gyre Heros}
%\setmonofont{TeX Gyre Cursor}


%\newfontfamily{\thaifont}[Scale=MatchUppercase,Mapping=tex-text]{TH Sarabun New}
%\newenvironment{thailang}
%{\thaifont}
%{}
%\usepackage[Latin,Thai]{ucharclasses}
%\setTransitionTo{Thai}{\begin{thailang}}
%	\setTransitionFrom{Thai}{\end{thailang}}
%\usepackage{setspace}
%\onehalfspacing
%\usepackage{polyglossia}
%\setdefaultlanguage{english}
%\setotherlanguage{thai}
%\newlength{\mylength}

\usepackage[top = 1.2in, bottom = 1in, left=1 in, right = 1in]{geometry}

\DeclareMathOperator\Arg{arg}
\DeclareMathOperator\cis{cis}
\DeclareMathOperator{\adj}{adj}

\newcommand{\dg}{^\circ}
\newcommand{\ii}{\item}
\newcommand{\ol}[1]{\overline{#1}}

\newcommand{\iv}{^{-1}}

\newcommand{\lbr}{\left\lbrace}
\newcommand{\rbr}{\right\rbrace}
\newcommand{\dps}{\displaystyle}

\newcommand{\CC}{\mathbb C}
\newcommand{\FF}{\mathbb F}
\newcommand{\NN}{\mathbb N}
\newcommand{\QQ}{\mathbb Q}
\newcommand{\RR}{\mathbb R}
\newcommand{\ZZ}{\mathbb Z}

\newcommand{\ra}{\rightarrow}
\newcommand{\lra}{\leftrightarrow}
\newcommand{\we}{\wedge}
\newcommand{\ve}{\vee}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
%\newtheorem{problem}{โจทย์ข้อที่}

\theoremstyle{remark}
%\newtheorem*{remark}{ข้อสังเกต}
\newtheorem*{note}{Note}

\pagestyle{fancy}%

\renewcommand{\chaptermark}[1]{%
	\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{%
	\markright{\thesection\ #1}{}}		

\providecommand\gi{}
% can be useful to refer to this outside \Set
\newcommand\SetSymbol[1][]{%
	\nonscript\:#1\vert
	\allowbreak
	\nonscript\:
	\mathopen{}}
\DeclarePairedDelimiterX\Set[1]\{\}{%
	\renewcommand\gi{\SetSymbol[\delimsize]}
	#1
}
\setlength{\headheight}{18pt}

\AtBeginDocument{	
	\DeclarePairedDelimiter\abs{\lvert}{\rvert}
	\DeclarePairedDelimiter\norm{\lVert}{\rVert}
	\DeclareMathOperator{\tr}{tr}
	
	\DeclareSymbolFont{AMSb}{U}{msb}{m}{n}
	\DeclareSymbolFontAlphabet{\mathbb}{AMSb}
	
	\DeclareSymbolFont{operators}   {OT1}{cmr} {m}{n}
	\DeclareSymbolFont{letters}     {OML}{cmm} {m}{it}
	\DeclareSymbolFont{symbols}     {OMS}{cmsy}{m}{n}
	\DeclareSymbolFont{largesymbols}{OMX}{cmex}{m}{n}
	\SetSymbolFont{operators}{bold}{OT1}{cmr} {bx}{n}
	\SetSymbolFont{letters}  {bold}{OML}{cmm} {b}{it}
	\SetSymbolFont{symbols}  {bold}{OMS}{cmsy}{b}{n}
	\DeclareSymbolFontAlphabet{\mathrm}    {operators}
	\DeclareSymbolFontAlphabet{\mathnormal}{letters}
	\DeclareSymbolFontAlphabet{\mathcal}   {symbols}
	\DeclareMathAlphabet      {\mathbf}{OT1}{cmr}{bx}{n}
	\DeclareMathAlphabet      {\mathsf}{OT1}{cmss}{m}{n}
	\DeclareMathAlphabet      {\mathit}{OT1}{cmr}{m}{it}
	\DeclareMathAlphabet      {\mathtt}{OT1}{cmtt}{m}{n}
}

%\makeatletter
%\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
%	\hskip -\arraycolsep
%	\let\@ifnextchar\new@ifnextchar
%	\array{#1}}
%\makeatother

\title{\textbf{Mathematics}}
\author{P. Tansuntorn}
\date{Last updated \today}
\begin{document}

	\frontmatter
	\pagestyle{plain}
	\maketitle
	\tableofcontents
	
	\pagestyle{fancy}
		\fancyhf{}%
		\fancyhead[RO,LE]{\thepage}
		\fancyhead[CO]{\textit{\rightmark}}
		\fancyhead[CE]{\textit{\leftmark}}
		\renewcommand{\headrulewidth}{0pt}
		\renewcommand{\headrulewidth}{0pt}
	
	\mainmatter
	\part{Part IA}
	\chapter{Numbers and Sets}
	\chapter{Groups}
		\section{Examples of Groups}
			\subsection{Axioms for groups}
				\begin{definition}
					A \textit{group} \index{Group} is a set $G$, together with a binary operation $\ast$ on $G$ with the following properties.
					\begin{enumerate}
						\item (Closure axiom) for all $g$ and $h$ in $G$, $g \ast h \in G$;
						\item (Associativity) for all $f, g$ and $h$ in $G$, $g \ast h \in G$, $f\ast(g\ast h) = (f \ast g) \ast h$;
						\item (Existence of identity) there is a unique $e$ in $G$ such that for all $g$ in $G$, $g \ast e = g = e \ast g$;
						\item (Existence of inverse) if $g \in G$ there is some $h$ in $G$ such that $g \ast h = e = h \ast g$.
					\end{enumerate}
				\end{definition}
			These results follow nicely.
			\begin{lemma}
	Let $G$ be any group. Then, given $g \in G$, there is only one element $h$ such that $g \ast h = e = h \ast g $. Particularly $(g^{-1})^{-1} = g.$
			\end{lemma}
			\begin{lemma}[Cancellation law]
				Suppose that $a,b$ and $x$ are in a group $G$. If $a \ast x = b \ast x$ then $a = b$.
		\end{lemma}
				\begin{lemma}
		Suppose that $a$ and $b$ are in a group $G$. Then the equation $a \ast x = b$ has a unique solution $x = a^{-1} \ast b$.
	\end{lemma}
		\begin{lemma}
			In any group $G$, $e$ is the unique solution of $x \ast x = x$.
		\end{lemma}
		Notice that we do not include the familiar assumption that $f \ast g = g \ast f$ normally found in arithmetic. In fact, for some interesting groups this equality does not hold.
		\begin{definition}
			Let $G$ be a group with respect to $\ast$. The elements $f$ and $g$ \textit{commute} if $f \ast g = g \ast f$. We call $G$ \textit{abelian} if for all $f$ and $g$ in $G$, we have $f \ast g = g \ast f$.
		\end{definition}
		We adopt the notation $gh$ as equivalent to $g \ast h$ for simplicity.
		
	\subsection{Examples from geometry}
		In this section we examine the idea of group in geometry, using polygons.
		
		
	\subsection{Permutation on a set}
				In this section we will show that permutations of a non-empty set $X$, in fact, form a group. 
				We start with the definition of permutations acting on a set, although only for finite sets, before developing the idea further into arbitrary sets.
				
				\begin{definition}
					A \textit{permutation} $\alpha \colon X \ra X$ is a bijection from $X$ to itself. We say that $\alpha$ acts on the set $X$. The set of all permutations of $X$ is denoted $\mathcal{P}(X)$.				
				\end{definition}
				This set is indeed a group.
				\begin{theorem}
					The set $\mathcal{P}(X)$ forms a group under composition of functions. We shall write $\alpha\beta(x)$ in place of $\alpha(\beta(x))$.
				\end{theorem}
				\begin{proof}
					We will show that all group axioms are satisfied.
					\begin{enumerate}
						\item It is obvious that if $\alpha, \beta$ are permutations, then $\alpha\beta$ is also a permutation. Thus the set $\mathcal{P}(X)$ is closed under composition.
						
						\item For any permutations $\alpha, \beta, \gamma$, let $\mu = \alpha\beta$ and $\nu = \beta\gamma$. Then for every $x$ in $X$,
						\begin{equation}
						\begin{aligned}
							(\alpha(\beta\gamma))(x) & = (\alpha\nu)(x) \\
									& = \alpha(\nu(x))	\\
									& = \alpha(\beta(\gamma(x))) \\
									& = \mu(\gamma(x))	\\
									& = (\mu\gamma)(x) \\
									& = ((\alpha\beta)\gamma)(x).
						\end{aligned}
						\end{equation}
						Thus the permutations are commutative under composition.
						
						\ii The identity permutation $\iota (x) = x$ is the identity of $\mathcal{P}(X)$, since $\alpha\iota(x) = \alpha(x) = \iota\alpha(x)$.
						
						\ii For any element $\alpha$ of $X$, the inverse is simply its functional inverse $\alpha^{-1}$. Direct verification shows that $\alpha\alpha^{-1} = \iota = \alpha^{-1}\alpha$.
					\end{enumerate}
				\end{proof}
				The above proof lets us write $\alpha\beta\gamma$ for any composition of three or more permutations without any confusion.
				
				Setting $X = \Set{1, \ldots, n}$, the study of permutation groups is simpler. We shall give a name for such group.
				\begin{definition}
					The \textit{symmetric group} \index{Symmetric group} $S_n$ is a set of permutations of $\Set{1, \ldots, n}$. We say that the group is of degree $n$.
				\end{definition}

				\begin{theorem}
					The order of $S_n$ is $n!$.
				\end{theorem}
				\begin{proof}
					Evidently, there are $n!$ permutations on a set with $n$ elements.
				\end{proof}
				We now introduce a customary notation for permutation $\rho(x)$ in the form
				\begin{equation*}
					\rho =
					\begin{pmatrix}
						1 & 2 & 3 & \cdots & n \\
						\rho(1) & \rho(2) & \rho(3) & \cdots & \rho(n)
					\end{pmatrix},
				\end{equation*}
				which mean that the image of the permutation $\rho(i)$ is underneath $i$ in the first row. For example, let $\alpha$ be a permutation on $\Set{1, 2, 3, 4}$ with $\alpha(1) = 1, \alpha(2) = 4, \alpha(3) = 2$ and $\alpha(4) = 3$, then
				\begin{equation*}
				\alpha =
				\begin{pmatrix}
					1 & 2 & 3 & 4\\
					1 & 4 & 2 & 3
				\end{pmatrix}.
				\end{equation*}
				\begin{example}
					There are 6 permutations in $S_3$, they are
					\begin{equation*}
					\begin{pmatrix}
					1 & 2 & 3\\
					1 & 2 & 3
					\end{pmatrix}, 
					\begin{pmatrix}
					1 & 2 & 3\\
					1 & 3 & 2
					\end{pmatrix}, 
					\begin{pmatrix}
					1 & 2 & 3\\
					2 & 1 & 3
					\end{pmatrix}, 
					\begin{pmatrix}
					1 & 2 & 3\\
					2 & 3 & 1
					\end{pmatrix}, 
					\begin{pmatrix}
					1 & 2 & 3\\
					3 & 1 & 2
					\end{pmatrix},
					\begin{pmatrix}
					1 & 2 & 3\\
					3 & 2 & 1
					\end{pmatrix}.
					\end{equation*}
				\end{example}
			Note that
			\begin{equation*}
				\begin{pmatrix}
					1 & 2 & 3 \\
					1 & 3 & 2
				\end{pmatrix}
				\begin{pmatrix}
				1 & 2 & 3 \\
				2 & 1 & 3
				\end{pmatrix}
				= \begin{pmatrix}
				1 & 2 & 3 \\
				3 & 1 & 2
				\end{pmatrix}
				\neq
				\begin{pmatrix}
				1 & 2 & 3 \\
				2 & 3 & 1
				\end{pmatrix}
				= 	\begin{pmatrix}
				1 & 2 & 3 \\
				2 & 1 & 3
				\end{pmatrix}
				\begin{pmatrix}
				1 & 2 & 3 \\
				1 & 3 & 2
				\end{pmatrix}.
			\end{equation*}
			Therefore $S_3$ is not abelian. More generally $S_n$ is not abelian for $n \geq 3$. We will show the sufficient condition that makes two permutations commute.
			
			\begin{definition}
				Any permutations $\alpha, \beta$ are said to be \textit{disjoint} if, for every $k$ in $\Set{1, 2, \ldots, n}$, either $\alpha(k) = k$ or $\beta(k) = k$.
			\end{definition}
			\begin{theorem}
				Two permutations commute if they are disjoint.
			\end{theorem}
			\begin{proof}
				Let the two permutations be $\alpha$ and $\beta$. For any $k \in \Set{1, \ldots, n}$, suppose that $\alpha$ fixes k, the case for $\beta$ can be argued similarly. 
				
				Let $\beta(k) = k'$. Then $\alpha\beta (k) = \alpha(k')$ and $\beta\alpha(k) = \beta(k) = k'$. We shall prove that indeed $\alpha(k') = k$.
				
				If $\beta(k') \neq k'$ then we are done by the premise. So suppose $\beta(k') = k'$, but then $\beta(k') = k' = \beta(k)$. This implies $k = k'$ and so $\alpha(k') = \alpha(k) = k'$ as required.
			\end{proof}
			We shall further simplify the notation, by introducing fixed points.
			\begin{definition}
				We call that $k$ is a \textit{fixed point} \index{Fixed point} of $\alpha$, and that $\alpha$ fixes $k$, if $\alpha(k) = k$.
			\end{definition}
			 And so, by convention, we shall left out any integers fixed by $\alpha$. For example, the permutation
			 \begin{equation*}
				 \alpha = \begin{pmatrix}
					 1 & 3 \\
					 3 & 1
				 \end{pmatrix}
			 \end{equation*}
			 interchanges 1 and 3, and fixes 2. This notation is still too cumbersome for large $n$, this drives us to find a new notation. Let us start by noticing that, if we repeatedly apply any permutation $\alpha$, any elements in $\Set{1, 2, \ldots, n}$ must eventually return. For example, let
			 \begin{equation*}
			 \alpha = \begin{pmatrix}
				 1 & 2 & 3 & 4 & 5\\
				 5 & 3 & 4 & 2 & 1
			 \end{pmatrix},
			 \end{equation*}
			 then $\alpha^2(1) = 1, \alpha^3(2) = 1, \alpha^3(3) = 3, \alpha^3(4) = 4$ and $\alpha^2(5) = 5$. This is easily proven using the pigeonhole principle. Notice that 1 and 5 \textit{form} a cycle between each other, as $\alpha$ sends 1 to 5 and also send 5 to 1; this is also the case for 2, 3, 4. The permutation $\alpha$ sends 2 to 3, 3 to 4, and 4 to 2. This is the motivation to define \textit{cycles}.
			 \begin{definition}
			 	A \textit{cycle} between $n_1, n_2, \ldots, n_q$ is the permutation
			 	\begin{equation*}
				 	\begin{pmatrix}
					 	n_1 & n_2 & \cdots & n_q \\
					 	n_2 & n_3 & \cdots & n_1
				 	\end{pmatrix}.
			 	\end{equation*}
			 	It is denoted by $(n_1 n_2 \cdots n_q)$. The cycle is said to be of length $q$.
			 \end{definition}
			 The integers $n_1, n_2, \ldots, n_q$ need not be in an increasing order. By inspection, $\alpha = (1 5)(2 3 4) = (2 3 4)(1 5)$. We will show that any permutation can be written in this manner, as the compositions of cycles.
			 
			 \begin{theorem}
			 	A permutation $\alpha$ in the symmetric group $S_n$ can be written as a composition of cycles.
			 \end{theorem}
	\subsection{Subgroups and homomorphisms}
		\begin{definition}
			A \textit{subgroup} of a group $G$ is a subset of $G$ which itself form a group under the operation taken from $G$.
		\end{definition}
		\begin{theorem}
			Let $H$ be a subgroup of $G$, then the identity element of $H$ is that of $G$.
		\end{theorem}
	
		A group $G$ always at least admits two subgroup, namely $G$ and the singleton $\Set{e}$. We call $\Set{e}$ the \textit{trivial subgroup} of $G$, and we say that $H$ is the \textit{non-trivial subgroup} of $G$ if $H \neq \Set{e}$. We say that $H$ is a \textit{proper subgroup} of $G$ if $H \neq G$.
		
		We now give a test for a subset to be a subgroup.
		
		\begin{theorem}[A test for subgroup]
			Let $G$ be a group, and $H$ be a non-empty subset of $G$. Then $H$ is a subgroup of $G$ if and only if
			\begin{enumerate}
				\item if $g \in H$ and $h \in H$, then $gh \in H$, and
				\item if $g \in H$ then $g^{-1} \in H$.
			\end{enumerate}
		\end{theorem}
		
		Another test is similar and follows from the above theorem.
		\begin{theorem}
			Let $G$ be a group, and $H$ be a non-empty subset of $G$. Then $H$ is a subgroup of $G$ if and only if $xy^{-1} \in H$ whenever $x,y \in H$.
		\end{theorem}
		
			The following property of the class of subsets of $G$ is important.
		\begin{theorem}
			Let $G$ be any group, then the intersection of any collection of subgroups of $G$ is itself a subgroup of $G$.
		\end{theorem}
			\begin{proof}
				Note that the intersection $\cap_t H_t$ of the subgroups of $G$, defined as $H_t$ for some $t$ in the index set $T$, is not empty. Then for every elements $g \in \cap_t H_t$ and $h \in \cap_t H_t$, they also lie in $H_t$ for every $t$. And thus $gh \in H_t$, so $gh \in \cap_t H_t$. Any element $g \in \cap_t H_t$ also has its inverse in every subgroup $H_t$. It then follows that $g^{-1} \in \cap_t H_t$. Therefore $\cap_t H_t$ forms a subgroup under the operation of $G$.
			\end{proof}
		
			As a consequence, we see that for any non-empty subset $G_0$ of $G$, we can consider the intersection of the collection of all subgroups $H$ of $G$ than contain $G_0$. The collection is not empty, since $G$ is itself in the collection. It follows that the intersection is itself not empty, and is a subgroup of $G$ that contain $G_0$. In fact, it is the \textit{smallest subgroup} to contain $G_0$. This allows us to propose the next definition.
			
			\begin{definition}
				Let $G_0$ be a non-empty subset of a group $G$. The subgroup of $G$ \textit{generated by} $G_0$ is the smallest subgroup of $G$ that contains $G_0$.
			\end{definition}
			
			The idea of subgroup is expanded into the notion of a \textbf{coset}, which will be explored later.
		\begin{definition}
			Let $G, G'$ be groups. A function $\phi \colon G \ra G'$ is a \textit{homomorphism} if it takes the action of $G$ to that of $G'$, namely
			\begin{equation*}
					\phi(gh) = \phi(g)\phi(h),
			\end{equation*}
			for all $g, h \in G$.
		\end{definition}
			
			
	\subsection{Symmetry groups}
		
	\subsection{The M\"obius group}
		We first begin with the definition of M\"obius transformation.
		\begin{definition}
			A \textit{M\"obius transformation} is a function $f$ of a complex variable $z$ in the form
			\begin{equation*}
				f(z) = \frac{az + b}{cz + d},
			\end{equation*}
			for some complex numbers $a, b, c$ and $d$, with the condition that $ad - bc \neq 0$.
		\end{definition}
		The condition $ad - bc \neq 0$ might not be obvious, but it follows from the fact that
		\begin{equation*}
			f(z) - f(w) = \frac{(ad - bc)(z - w)}{(cz+d)(cw + d)}.
		\end{equation*}
		If $ad - bc = 0$, then $f$ is constant. This also shows that $f$ is injective.
		
		This definition of the M\"obius transformation has two problems. First, a M\"obius transformation $f$ is not unique. As for example, the 4-tuples $(a,b,c,d)$ and $(ma, mb, mc, md)$ with $m \neq 0$ will all map a complex number $z$ to a same number. Thus, given $f$, we \textit{cannot} say what are the coefficients.
		
		The second problem stems from the fact that, for example $1/(z - z_0)$ is not defined at the point $z_0$. This means that there is no subset of $\CC$ on which all M\"obius maps are defined.
		
		Here is an example of this.
		\begin{example}
			Let $f(z) = (z + 2)/ z$ and $g(z) = (z + 1)/(z - 1)$. Then,
			\begin{equation*}
				f(g(z)) = \frac{g(z) + 2}{g(z)} = \frac{(z + 1)+ 2(z-1)}{z+ 1} = \frac{3z - 1}{z + 1},
			\end{equation*}
			so that $fg$ fixes the point $1$. However, $g$ \textit{is not defined when} $z = 1$. What's worse is that, if $h(z) = 1/z$ then $hfg(z) = (z+1)/(3z - 1)$, although $g$ is not defined when $z = 1$, $fg(z)$ is not defined when $z = -1$, and $hfg(z)$ is not defined when $z = 1/3$. More generally, a composition $f_1 \cdots f_n$ of M\"obius maps will not be defined at $n$ distinct points in the complex plane.
		\end{example}
		The following theorem addresses the first problem.
		\begin{theorem}
			Suppose that $a, b, c, d, \alpha, \beta, \gamma \text{ and } \delta$ are complex numbers with ${(ad - bc)}{(\alpha\delta - \beta\gamma)}{ \neq 0}$, and such that for at least three distinct values of $z$ in $\CC$, $cz + d \neq 0, \gamma z+ \delta \neq 0$, and
			\begin{equation*}
				\frac{az + b}{cz + d} = \frac{\alpha z + \beta}{\gamma z + \delta}.
			\end{equation*}
			Then there is some non-zero complex number $\lambda$ such that
			\begin{equation}
				\begin{pmatrix}
					\alpha & \beta \\
					\gamma & \delta
				\end{pmatrix}
				= \lambda\begin{pmatrix}
					a & b \\
					c & d
				\end{pmatrix}.
			\end{equation}
		\end{theorem}
		\begin{proof}
			Consider the quadratic polynomial
			\begin{equation*}
				(az + b)(\gamma z + \delta) = (\alpha z + \beta)(cz + d).
			\end{equation*}
			The polynomial has three distinct roots, and so it must be a zero polynomial. Therefore, $a\gamma = c\alpha, b\gamma + a \delta = c\beta + d\alpha$ and $b\delta = d\beta$, which is equivalent to
			\begin{equation*}
				\begin{pmatrix}
					d & -b\\
					-c & a
				\end{pmatrix}
				\begin{pmatrix}
				\alpha & \beta \\
				\gamma & \delta
				\end{pmatrix}
				=
				\begin{pmatrix}
					\mu		&	0\\
					0		& \mu
				\end{pmatrix},
			\end{equation*}
			where $\mu^2 = {(ad - bc)}{(\alpha\delta - \beta\gamma)}{ \neq 0}$. We then have
			\begin{equation*}
				\begin{pmatrix}
				\alpha & \beta \\
				\gamma & \delta
				\end{pmatrix}
				= \frac{\mu}{ad - bc}\begin{pmatrix}
				a & b \\
				c & d
				\end{pmatrix}.
			\end{equation*}%
		\end{proof}
		The first problem is then resolved by showing that the 4-tuple $(a,b,c,d)$ determines $f$, up to non-zero multiple. The second problem will be resolved differently, by joining an extra point, which is called \textit{the point at infinity} to $\CC$. This point is denoted $\infty$.
	
	
		\section{Lagrange's Theorem}
			\subsection{Cosets}
				We have introduced the idea of subgroup in the previous section. Now we come to the idea of constructing a subset of any group $G$ from its subgroup. For example, we could define a new subset $XY$ of $G$ by
				\begin{equation*}
					XY = \Set{xy \colon x \in X, y\in Y}
				\end{equation*}
				for any subgroup $X, Y$ of $G$. If $X$ is a singleton, that is $X = \Set{x}$, we shall adopt a notation $XY = xY$. Such constructions which we shall consider are of the form
				\begin{equation*}
					gH = \Set{gh \colon h\in H} \text{ or } Hg = \Set{hg \colon h \in H}
				\end{equation*}
				for some $g \in G$, and $H$ is a subgroup of $G$. The set $gH$ is called the \textit{left coset} of $H$ with respect to $g$, similarly, $Hg$ is the \textit{right coset} of $H$ with respect to $g$. Some constructions of this type might turn out to be the same set $H$. This is illustrated below.
				
				\begin{theorem}
					Let $H$ be a subgroup of $G$, and $g \in G$. Then $g \in H$ if and only if $gH = H$ (or $Hg = H$).
				\end{theorem}
				
				Thus we concern ourselves to the study of $gH$ when $g \notin H$. We will adopt an additive notation $g + H$ in place of $gH$ when such subgroups employ addition. The next results show that a group can be divided into disjoint cosets. This is called the \textit{coset decomposition of $G$}.
				
				\begin{theorem}
					Let $H$ be a subgroup of a group $G$, then $G$ is a union of its left (or right) cosets.
				\end{theorem}
				\begin{proof}
					Clearly, for any $g \in G$, $g \in gH$. So $g$ is contained in the union.
				\end{proof}
			
				\begin{theorem}
					Let $H$ be a subgroup of a group $G$, then any two left cosets of $G$ are either equal or disjoint.
				\end{theorem}
				\begin{proof}
					Let $f, g \in G$ and $fH, gH$ are the two left cosets. Suppose that $fH$ and $gH$ are disjoint, that is, the set $fH \cap gH$ is not empty. Then there exists an element $x \in fH \cap gH$, and so $fy_1 = gy_2$ for some $y_1, y_2 \in H$. Thus $g^{-1}f = y_2y^{-1}_1 \in H$ and so $g^{-1}fH = H$; hence $gH = g g^{-1} f H = fH$, hereby proving the theorem.
				\end{proof}
				\begin{corollary}
					If $fH = gH$, then $g^{-1} f \in H$.
				\end{corollary}
				
			\subsection{Lagrange’s theorem}
			Recall the definition of an \textit{order} of a group, denoted $\abs{G}$. The next theorem shows the connection between the orders of a group and its subgroup.
				\begin{theorem}[Lagrange's theorem]
					Let $H$ be a subgroup of a finite group $G$. Then $\abs{H}$ divides $G$, and $\abs{G}/\abs{H}$ is the number of distinct left (or right) cosets of $H$ in $G$.
				\end{theorem}
				\begin{proof}
					From the previous theorem we can write a group $G$ as a union of the pairwise disjoint coset left of $H$. Therefore $G = g_1H \cup g_2 H \cup \cdots \cup g_r H$. Consequently,
					\begin{equation*}
						\abs{G} = \abs{g_1 H} + \abs{g_2 H} + \cdots + \abs{g_r H}.
					\end{equation*}
					It remains to show that $\abs{g_1 H} = \abs{g_2 H} = \cdots = \abs{g_r H} = \abs{H}$. Notice that the map $x \mapsto g_j x$ is a bijection from $H$ to $g_j H$, and so $\abs{g_1 H} = \abs{g_2 H} = \cdots = \abs{g_r H} = \abs{H}$. Therefore $\abs{G} = r\abs{H}$ and the results follow.
				\end{proof}
			
			One of the corollary of Lagrange's theorem is the following result.
			\begin{corollary}
					Let $g$ be an element of a finite group $G$. Then the order of $g$ divides the order of $G$.
			\end{corollary}
			\begin{proof}
				Let $d$ be the order of $g$. The subgroup $H = \Set{e, g, g^1, \ldots, g^{d-1}}$ is a subgroup of order $d$. By Lagrange's Theorem, $\abs H \mid \abs G$.
			\end{proof}
			\subsection{Group of small order (up to 8)}
			
			\subsection{Quaternions}
			
			\subsection{Fermat-Euler theorem}
			
		\section{Group actions}
		This section studies \textbf{group actions}.
			\subsection{Group actions}
			
			\subsection{Orbit-stabilizer theorem}
			
			\subsection{Cayley's theorem}
			
			\subsection{Conjugacy classes}
			
			\subsection{Cauchy's theorem}
			
		\section{Quotient groups}
			\subsection{Normal subgroups}
			
			\subsection{Quotient groups}
			
			\subsection{The isomorphism theorem}
			
		\section{Matrix groups}
			\subsection{The general and special linear groups}
			
			\subsection{The orthogonal and special orthogonal groups}
			
			\subsection{Basis change}
		\section{Permutations}
			\subsection{Permutations, Cycles and Transpositions}
			
			\subsection{Sign of Permutations}
			
			\subsection{Conjugacy in $S_n$ and $A_n$}
			
			\subsection{Simple Groups}

	\chapter{Vectors and Matrices}
%		\section{Complex Numbers}
		
		\section{Vectors}
			\subsection{Vector Algebra in $\mathbb{R}^3$}
				Combining two vectors
			
			\subsection{Vectors in $\RR^n$ and $\CC^n$}
				Let us consider the vector in $\RR^n$, the natural generalisation of $\RR^3$.
				\begin{definition}
					Using the standard basis $e_1, \ldots, e_n$ of $\RR^n$, if $x = \sum_j x_j e_j$ and $y = \sum_j y_j e_j$, we write
					\begin{equation*}
						x \cdot y = \sum_{j = 1}^{n} x_j y_j, \norm{x}^2 = x \cdot x = \sum_{j = 1}^{n} x^2_j,
					\end{equation*}
					and $x \perp y$ when $x\cdot y = 0$.
				\end{definition}
			Note that $\norm x = \norm {-x}$. The distance $\norm{x - y}$ between the points $x$ and $y$ is given by the natural extension of Pythagoras' theorem, and importantly, satisfies the \textit{triangle inequality}.
			\begin{theorem}[The triangle inequality for $\RR^n$]
				For all $x, y, z$ in $\RR^n$,
				\begin{equation}
					\norm{x - y} \leq \norm{x - y} + \norm{y - z}.
				\end{equation}
			\end{theorem}
			To prove this assertion, it is sufficient to show that $\abs{x \cdot y} \leq \norm x \norm y$, so that we have $\norm{x + y} \leq \norm x + \norm y$, which readily implies the triangle inequality. Thus we seek to prove
			\begin{theorem}[the Cauchy-Schwarz inequality]
				For all $x, y \in \RR^n$,
				\begin{equation}
					\abs{x \cdot y} \leq \norm x \norm y.
				\end{equation}
				The equality holds if and only if $\norm x y = \pm \norm y x$, i.e. one vector is a multiple of one another. 
			\end{theorem}
			\begin{proof}
				Let $x = (x_1, \ldots, x_n)$ and $y = (y_1, \ldots, y_n)$. The equation holds true when $x = 0$ and when $y = 0$. So we assume that $\norm x \norm y > 0$.
				
				Consider the equation
				\begin{equation*}
					0 \leq \sum_{j = 1}^n \left(\norm x y_j - \norm y x_j\right)^2 = 2\norm x \norm y \left(\norm x \norm y - xy\right),
				\end{equation*}
				so $x \cdot y \leq \norm x \norm y$; similarly, put $ - x $ as $x$ and we have $- x \cdot y \leq \norm x\norm y$. Therefore $\abs{x \cdot y} \leq \norm x \norm y$. Equality holds if $\sum_{j = 1}^n \left(\norm x y_j - \norm y x_j\right)^2$ or $\sum_{j = 1}^n \left(\norm x y_j + \norm y x_j\right)^2$ is equal to zero, which implies $ \norm x y = \pm \norm y x$.
			\end{proof}
		
			\subsection{Concepts in linear algebra}
		\section{Matrices}
			\begin{definition}
				An $n \times m$ matrix is an array of numbers of the form
				\begin{equation*}
					\begin{pmatrix}
					a_{11} & a_{12} & \ldots & a_{1m} \\
					a_{21} & a_{22} & \ldots & a_{2m} \\
					\vdots & \vdots & \ddots & \vdots \\
					a_{n1} & a_{n2} & \ldots & a_{nm}
					\end{pmatrix}.
				\end{equation*}
				Sometimes it will be denoted by $(a_{ij})$, where $a_{ij}$ is the general element of the matrix.
			\end{definition}
				Notice that $i$ is the \textit{row} of the element, and $j$ is the \textit{column} of the element.
			\subsection{Algebra of Matrices}
			
			\subsection{Matrix as linear transformation}
				We start with the definition of linear transformations.
				\begin{definition}
					A map $\alpha \colon V \ra W$ between vector spaces $V$ and $W$ is \textit{linear} if, for all scalars $\lambda_1, \ldots, \lambda_n,$ and all vectors $v_1, \ldots, v_n$,
					\begin{equation*}
						\alpha\left(\lambda_1v_1 + \cdots + \lambda_n v_n\right) = \lambda_1\alpha(v_1) + \cdots + \lambda_n\alpha(v_n).
					\end{equation*}
				If $\alpha$ is linear we say that it is a \textit{linear transformation}, or a \textit{linear map}, if for all scalars $\lambda$ and all vectors $u$ and $v$, $\alpha(\lambda x) = \lambda\alpha(x)$ and $\alpha( x + y) = \alpha (x) + \alpha(y)$.
				\end{definition}
			The two definitions are equivalent.
				\begin{theorem}[Rank-nullity theorem]
					content...
				\end{theorem}
			
			\section{Eigenvalues and Eigenvectors}
				
	\chapter{Differential Equations}
		\section{Basic Calculus}
		
		\section[1st-order LDEs]{First-order Linear Differential Equations}
			\subsection{Equations with constant coefficients}
				
			\subsection{Equations with non-constant coefficients}
			
		\section{Nonlinear first-order equations}
		
		\section[Higher-order LDEs]{Higher-order Linear Differential Equations}
		
		\section{Multivariate Functions}
	
	\chapter{Analysis I}
		A rigorous theory of mathematical analysis must take an axiomatic approach as its foundation. Thus it is preferable to start from the construction of real numbers, and then discover their properties, as not to take them for granted. This foundational rigour is, fortunately, available for us by Dedekind and his model for the real number.
		
		What are the essential properties of $\RR$? We have learnt that $\RR$ is a field, with the usual addition and multiplication; the usual subtraction and division is also possible.
		
		Secondly, there is a \textit{total order} on $\RR$, that is, if $x, y \in \RR$ then either $x \leq y$ or $y \leq x$, and only $x = y$ when both condition are satisfied. Furthermore, if $x \leq y$ and $y \leq z$ then $x \leq z$. This means $\RR$ is an \textit{ordered field} and that is, if $x \leq y$ then $x + z \leq y + z$, and if $w \geq 0$ then $xw \leq yw$.
		
		Of course, $\QQ$ is also an ordered field, but it is not \textit{complete}. This is the most important property of $\RR$ to keep in mind. Let's start by a notion of an \textit{upper bound}. If $A$ is a non-empty subset of $\RR$ and $b \in \RR$, then $b$ is an upper bound for $A$ if $b \geq a$ for all $a \in A$. By saying that $\RR$ is complete, this means that, if $A$ is a non-empty set of $\RR$ with an upper bound, then $A$ has a \textit{least upper bound}, or \textit{supremum} $\sup A$. This translates to, for any upper bound $b$ of a set $A \subset \RR$, should it exist, we have $\sup A \leq b$.
		
		Another central theme of analysis regards \textit{absolute value}, that is the function
		\begin{equation}\abs{x} = 
		\begin{cases*}
			x & \text{ if $x \geq 0$}\\
			0 & \text{ if $x = 0$} \\
			-x & \text{ if $x \leq 0$}
		\end{cases*}.
		\end{equation}
		
		Note that $\abs{x - y} = \abs{y - x}$ and $\abs{x} \geq 0$ for all $x \in \RR$. 
		\begin{theorem}
			For all $x, y \in \RR$, $\abs{x + y} \leq \abs{x} + \abs{y}$, with equality when $ xy \geq 0$.
		\end{theorem}
		\begin{proof}
			Trivial proof by case.
		\end{proof}
		\begin{theorem}(Triangle Inequality)
			For all $x, y, z \in \RR$, we have
			\begin{equation}
				\abs{x - z} \leq \abs{x - y} + \abs{y - z}.
			\end{equation}
		\end{theorem}
		\begin{proof}
			Simply substitute $x - y$ and $y - z$ in place of $x$ and $y$, respectively.
		\end{proof}
		\section{Limit and Convergences}
			\subsection{Series and sequences in $\mathbb{R}$ and $\mathbb{C}$}
				\begin{definition}
					A sequence $s_n$ is a \textit{null sequence} if, to every positive number $\epsilon$, there corresponds an integer $N$ such that
					\begin{equation*}
						\abs*{s_n} < \epsilon \text{ for all values of $n$ greater than } N.
					\end{equation*}
				\end{definition}
			
			We can adapt the definition to any sequence whose terms approach any number $s$.
			
				\begin{definition}
					A sequence $s_n$ is said to tend to the limit $s$ if, given any positive number $\epsilon$, there is an integer $N$ (depending on $\epsilon$) such that
						\begin{equation*}
							\abs*{s_n - s} < \epsilon \text{ for all } n > N.
						\end{equation*}
						We then write $\lim s_n = s$.
				\end{definition}
			  A more clear notation $\lim_{n \ra \infty} s_n = s$ can be given.
			\begin{note}
				\begin{enumerate}
					\item Clearly, $\lim s_n = s$ if and only if $s_n - s$ is a null sequence.
					
					\item The inequality $\abs*{s_n - s} < \epsilon$ is equivalent to the two inequalities
					\begin{equation*}
					s - \epsilon < s_n < s + \epsilon.
					\end{equation*}
					This is clear that $s_n$ is bounded.
					
					\item A short notation $s_n \ra s$ stands for $\lim s_n = s$. A further symbolism for the above definition may be given:
					\begin{equation*}
					s_n \ra s \text{ if } \epsilon >0;\quad \exists N.\abs*{s_n - s} < \epsilon \text{ for all } n > N.
					\end{equation*}
				\end{enumerate}
			\end{note}
		    If limits exist, they are unique.
		    \begin{theorem}
		    	If $a_n \ra s$ as $n \ra \infty$ and $a_n \ra l$ as $n \ra \infty$, then $s = l$.
		    \end{theorem}
	    	\begin{proof}
	    		We will prove this theorem by contradiction. Suppose $s \neq l$. Let $\epsilon = \abs{s - l}/3 > 0$. There exists $n_0$ such that $\abs{a_n - s} < \epsilon$ for $n \geq n_0$, and there exists $m_0$ such that $\abs{a_n - l} < \epsilon$ for $n \geq m_0$. Let $N = \max(n_0 , m_0)$. Then if $n \geq N$,
	    		\begin{equation*}
	    			\abs{l - s} \leq \abs{a_n - l} + \abs{a_n - s} < 2\epsilon = 2\abs{l - s}/3,
	    		\end{equation*}
	    		a contradiction.
	    	\end{proof}
    		We have discussed on upper bound and lower bound of a set, it is time to introduce a notion of \textit{boundedness}, and expand it to those of sequences in general.
    		\begin{definition}
    			A subset $A$ of $\RR$ is \textit{bounded} if it is bounded above and bounded below. A sequence $s_n$ is bounded if the set $\Set{s_n \colon n \in \ZZ^+}$ is bounded.
    		\end{definition}
    		\begin{theorem}
    			If a sequence tends to a limit, then it is bounded.
    		\end{theorem}
    		\begin{proof}
    			Let the sequence $a_n$ tends to the limit $l$. We choose an arbitrary $\epsilon$ so that for any $n \geq n_0$ the difference $\abs{a_n - l}$ is less than $\epsilon$. 
    			
    			Let $\epsilon = 1$, so that $\abs{a_n - l} < 1$ for all $n \geq n_0$. Choose
    			\begin{equation*}
    				M = \max\Set{\abs {a_1} , \abs {a_2}, \ldots, \abs {a_{n_0}}, \abs{l} + 1}.
    			\end{equation*}
    			Then for all $n \geq n_0$ $\abs{a_{n}} \leq \abs{a_{n} - l} + \abs{l} < 1 + \abs{l}$. Clearly, $\abs{a_n} \leq M$ and we are set.
    		\end{proof}
    	Note that the converse of the theorem might not be true; if a sequence is bounded, then it \textit{might not} tends to a limit. Consider the sequence $a_n = \cos n\pi$. It is bounded, but $a_n$ does not tend to a limit.
    		\begin{theorem}
    			Suppose that $a_n$ is an increasing sequence of real numbers. If it is bounded then $a_n \ra \sup\Set{a_n \colon n \in \ZZ^+}$ as $n \ra \infty$; otherwise $a_n \ra +\infty$.
    			
    			Similarly, for any decreasing sequence $a_n$; and if it is bounded, then $a_n \ra {\inf\Set{a_n \colon n \in \ZZ^+}} $; otherwise $a_n \ra -\infty$.
    		\end{theorem}
    	
    		One sequence worth considering is the sequence $a_n = r^n$. The convergence of the sequence depends on the value of $r$.
    		\begin{enumerate}
    			\item If $r = 1$ then $a_n \ra 1$, and if $r = 0$ then $a_n \ra 0$.
    			
    			\item If $r > 1$ then $r = 1 + k$ for some $k > 0$, so we have
    			\begin{equation*}
	    			a_n = \left(1 + k\right)^n > 1 + kn
    			\end{equation*}
    			by considering the first two terms in the binomial expansion. And so $a_n \ra +\infty$.
    		\end{enumerate}
    		\subsection{Sums, products and quotients}
    			\begin{theorem}
    				If $s_n$ and $t_n$ are null sequences, so is $s_n + t_n$.
    			\end{theorem}
    		
    			\begin{theorem}
    				If $s_n$ is a null sequence and $t_n$ is a bounded sequence, then $s_n t_n$ is a null sequence.
    			\end{theorem}
    		
    			\begin{corollary}
    				If $s_n$ is a null sequence and $c$ is a constant, then $cs_n$ is a null sequence.
    			\end{corollary}
    				We then now extend the results to general sequences.
    			\begin{theorem}
    				If $s_n \ra s$ and $t_n \ra t$, then
    				\begin{enumerate}
    					\item $s_n + t_n \ra s + t$,
    					\item $s_n t_n \ra st$.
    				\end{enumerate}
    			\end{theorem}
    		
    			\begin{theorem}
    				If $s_n \ra s$ and $t_n \ra t$ with $t \neq 0$, then
    				\begin{equation*}
    					\frac{s_n}{t_n} \ra \frac{s}{t}
    				\end{equation*}
    			\end{theorem}
    			
    			\begin{theorem}
    				If $s_n \ra s$ and $t_n \ra t$ and $s_n \leq b_n$ for all $n$, then $a \leq b$.
    			\end{theorem}
    		
    			\begin{theorem}
    				If $s_n \ra s$ and $s_{n_k}$ is a subsequence, then $s_{n_k} \ra s$.
    			\end{theorem}
    		
			\subsection{Absolute convergence}
			
			\subsection{Bolzano-Weierstrass theorem}
				\begin{theorem}(Bolzano-Weierstrass theorem)
					Suppose that $a_n$ is a bounded sequence of real numbers. There there is a subsequence $a_{n_k}$ which converges.
				\end{theorem}
			\subsection{Comparison and ratio test}
			
			\subsection{Alternating series test}

		\section{Continuity}
			\subsection{Continuity of real and complex function}
			
			\subsection{The intermediate value theorem}
			
		\section{Differentiability}
			\subsection{Differentiability of functions from $\RR$ to $\RR$}
			
			\subsection{Derivative of sums and products}
			
		\section{Power series}
			\begin{definition}
				An infinite series of the form
				\begin{equation*}
					\sum_{n = 0}^{\infty} a_n z^n = a_0 + a_1 z + a_2 z^2 + \cdots
				\end{equation*}
				composed of multiples of powers of $z$ is called a \textit{power series}. Both the variable $z$ and the coefficients $a_n$ might be real of complex.
			\end{definition}
			There are three possibilities with convergence of a power series.
			\begin{enumerate}
				\item The series converges for all $z \in \CC$.
				
			\end{enumerate}
		

		
		\section{Integration}
			\subsection{Integrability of monotonic functions}
	
	\chapter{Probability}
		\section{Basic concepts}
		
		\section{Axiomatic approach}
		
		\section{Discrete random variables}
		
		\section{Continuous random variables}
		
		\section{Inequalities and limits}
			\subsection{Markov's and Chebyshev's inequality}
			
			\subsection{Weak law of large numbers}
			
			\subsection{Convexity and Jensen's inequality}
			
			\subsection{AM-GM inequality}
	
	\chapter{Vector Calculus}
		\section{Curves in $\RR^3$}
		
	\part{Part IB}
	\chapter{Linear Algebra}
		\section{Vector Spaces}
		
		\section{Linear maps}
		
		\section{Determinant}
		
		\section{Eigenvalues and Eigenvectors}
		
		\section{Duals}
		
		\section{Bilinear Forms}
		
		\section{Inner Product Spaces}
		
		
	\chapter{Groups, Rings and Modules}
		\section{Groups}
		We have gone into details of groups in Part IA.
			\subsection{Basics concepts}
			
			\subsection{Normal subgroups}
			
			\subsection{Sylow subgroups and Sylow theorems}
		\section{Rings}
			\subsection{Definition}
			Rings are abstraction of systems with addition and multiplication. The prototype of rings are the set $\ZZ$ of integers.
			
			We define the general notion of ring in a similar way. We say that a set $R$ with two operations, addition and multiplication, denoted $x + y$ and $x \cdot y$, respectively.
			We write $x \cdot y$ as $xy$ for comprehensiveness.
			\begin{definition}
				A set $R$ is a ring if the following properties are satisfied:
				\begin{enumerate}
					\item $R$ forms an abelian group under addition.
					\item $R$ forms a monoid under multiplication.
					\item The distributive laws hold true, i.e.
					\begin{equation*}
						x(y + z) = xy + xz, (y + z)x = yx + zx.
					\end{equation*}
				\end{enumerate}
			\end{definition}
			\subsection{Ideals}
			
			\subsection{Fields}
			
			\subsection{Factorisation in rings}
			
			\subsection{Rings $\ZZ[a]$ of algebraic integers}
		\section{Modules}
			\subsection{Definition}
			
			\subsection{Submodules}
			
			\subsection{Equivalence of matrices}
			
			\subsection{Finitely generated modules over Euclidean domains}
	\chapter{Analysis II}
		\section{Uniform Convergence}
		
		\section{Uniform Continuity and Integration}
		
		\section{$\RR^n$ as a Normed Space}
		
		\section{Differentiation from $\RR^m$ to $\RR^n$}
		
		\section{Metrice Spaces}
		
		\section{The Contraction Mapping Theorem}
		
	\chapter{Metric and Topological Spaces}
		\section{Metrics}
			\subsection{Definition and examples}
			
			\subsection{Limits and continuity}
			
			\subsection{Open sets and neighbourhoods}
			
			\subsection{Characterising limits and continuity}
			
		\section{Topology}
			\subsection{Metric topologies}
			
		\section{Connectedness}
		
		\section{Compactness}
		
	\chapter{Complex Analysis}
		\section{Analytic Functions}
		
		\section{Contour Integration and Cauchy's Theorem}
		
		\section{Expansions and Singularities}
		
		\section{The Residue Theorem}
		
	\chapter{Complex Methods}
		\section{Analytic Functions}
		
		\section{Contour Integration and Cauchy's Theorem}
		
		\section{Residue Calculus}
		
		\section{Fourier and Laplace Transforms}
	
	\chapter{Geometry}
		
	
	\part{Part II}
	\chapter{Number Theory}
		\section{Basics}
		
		\section{Chinese Remainder Theorem}
		
		\section{Law of Quadratic Reciprocity}
		
		\section{Binary Quadratic Forms}
		
		\section{Distribution of the Primes}
		
		\section{Continued fractions and Pell's equation}
		
		\section{Primality testing}
		
		\section{Factorisation}
	\chapter{Topics in Analysis}
	
	
	\chapter{Coding and Cryptography}
	
	\chapter{Automata and Formal Languages}
		\section{Register machines}
		
		\section{Regular languages and finite-state automata}
		
		\section{Pushdown automata and context-free languages}
	\chapter{Logic and Set Theory}
		\section{Ordinals and Cardinals}
			\subsection{Well-orderings and order-types}
			
		\section{Posets and Zorn's Lemma}
		
		\section{Propositional Logic}
		
		\section{Predicate Logic}
		
		\section{Set Theory}
		
		\section{Consistency}
		
	\chapter{Graph Theory}
		\section{Introduction}
		
		\section{Connectivity and matchinhs}
		
		\section{Extremal graph theory}
		
		\section{Eigenvalue methods}
		
		\section{Graph colouring}
		
		\section{Ramsey theory}
		
		\section{Probabilistic methods}
	\chapter{Galois Theory}
		\section{Fields extensions}
	\chapter{Representation Theory}
		\section{Representations of Finite Groups}
			\subsection{Representations on vector spaces}
		
		\section{Character Theory}
		
		\section{Arithmetic Properties of Characters}
		
		\section{Tensor Products}
		
		\section{Representations of $S^1$ and $SU_2$}
		
		\section{Further Worked Examples}
		
	\chapter{Number Fields}
		\section{Algebraic Number Fields}
		
		\section{Ideals}
		
		\section{Units}
		
		\section{Ideal classes}
		
		\section{Dedekind’s theorem on the factorisation of primes}
	\chapter{Algebraic Topology}
		\section{The Fundamental Group}
		
		\section{Covering Spaces}
		
		\section{The Seifert-Van Kampen Theorem}
		
		\section{Simplicial Complexes}
		
		\section{Homology}
		
		\section{Homology Calculations}
		
	\chapter{Linear Analysis}
	
	\chapter{Analysis of Functions}
	
	\chapter{Riemann Surfaces}
	
	\chapter{Algebraic Geometry}
	
	\chapter{Differential Geometry}
	
	\chapter{Probability and Measure}
	
	\backmatter
		\printindex
%		\begin{equation*}
%			(9)^{2558} = (10 - 1)^{2558} = \binom{2558}{0}10^{2558} - \binom{2558}{1}10^{2557}\cdot 1 + \binom{2558}{2}{10}^{2557}\cdot 1^2 - \cdots - \binom{2558}{2557}5\cdot 1^{2557} + \binom{2558}{2558}1^{2558} \\
\end{document}
		